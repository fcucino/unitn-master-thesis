\chapter{Background}
\label{cha:background}

% Software vs. Security
% Risk and vulnerability and CVEs
% Industrial Control Systems (ICS) and Operational Technology (OT)
% Industrial Internet of Things (IIoT)
% Difference Between IT and OT Security
% Devices and Constraints
% What Is a Security Scan

This chapter contains an explanation of many fundamental terms used in the thesis, without which it would not be possible to fully understand all the covered topics.

\section{Software vs. Security}

A software is a set of programs and data which provides functionalities. Security is understanding and identifying software-induced security risks and how to manage them. Software and functionalities come with certain risks and software security is about managing these ones.

Therefore, software plays a crucial role in providing security, but it is also one of the relevant sources of security issues and problems. Many times the developers have a limited training on this concept or many times the goal is the speed of the development rather than the attention to the hidden details; as result, security is always considered a secondary factor, because it is a complex and expansive task, hard to evaluate when nothing bad happens but usually too late to evaluate when something bad happens.~\cite{st-slides}

A software system is secure if it satisfies specified security objectives, starting from the CIA triad:
\begin{itemize}
  \item \textbf{Confidentiality}: unauthorized actors cannot have access or read the information;
  \item \textbf{Integrity}: unauthorized actors cannot change or alter the information;
  \item \textbf{Availability}: authorized actors can always have access to the information;
\end{itemize}
The CIA triad is a common model that stands as basis for the development of security systems.
% it helps guiding teams as they pinpoint the different ways in which they can address each concern.\\
Ideally, when all three standards have been met, the security profile of the organization is stronger and better equipped to handle threat incidents.~\cite{cia-triad}

Secure software is not only composed of the CIA triad; there are also other objectives like~\cite{st-slides}
\begin{itemize}
  \item \textbf{Authentication}: who is performing a task;
  \item \textbf{Authorization}: what is that actor allowed to do;
  \item \textbf{Privacy}: controlling personal information from being shared;
  \item \textbf{Anonymity}: remaining unidentified to others;
  \item \textbf{Non-repudiation}: actor cannot deny having taken an action;
  \item \textbf{Reliability}: the extent to which a software yields consistent and results as expected;
  \item \textbf{Audit}: having traces of performed actions in separate systems or places;
  \item \textbf{Monitoring}: observing the system for any unusual activity;
  \item \textbf{Intrusion detection}: detecting unauthorized access;
  \item \textbf{Intrusion prevention}: stopping unauthorized access;
  \item ...and so on
\end{itemize}

Total security is unachievable, but the goal is to minimize the risks and the vulnerabilities. The security is a process, not a product, and it is a continuous process.~\cite{st-slides}

\section{Risk vs. vulnerability vs. CVE}

We talked about risks in the previous section, but what is a risk? A risk is the potential that a dangerous situation becomes reality. It is the probability of a threat exploiting a vulnerability and the impact of that event. A risk is a combination of a threat, a vulnerability and an impact.

Safety is about protecting from accidental risks, while security is about mitigating the risk of dangers caused by intentional and malicious actors.

A system is secure as its weakest element. A vulnerability is a weakness in a system that can be exploited by a threat. A threat is a potential danger that can exploit a vulnerability. A threat agent is the actor that can exploit a vulnerability. An attack is the exploitation of a vulnerability by a threat agent. An exploit is the code that takes advantage of a vulnerability.

CVE, which stands for \textit{Common Vulnerabilities and Exposures}, is a list of publicly known cybersecurity vulnerabilities. The CVE system provides a reference method for publicly known information-security vulnerabilities and exposures. Each CVE entry represents a unique identifier for a specific vulnerability, named \textit{CVE-ID}, making it easier to reference it across different tools.

Each CVE entry is evaluated using the CVSS score, which stands for \textit{Common Vulnerability Scoring System}. This system provides a standardized way to assess the severity of vulnerabilities. It is important to note that CVSS measures severity, not risk. CVSS v2.0 and CVSS v3.x include three metric groups: Base, Temporal and Environmental. The latest version, CVSS v4.0, introduced in 2023, includes Base, Threat, Environmental and Supplemental metric groups. The Base metrics evaluate the intrinsic characteristics of a vulnerability that are constant over time and across user environments. The Temporal metrics adjust the valutation based on factors like patch availability and exploit code. The Environmental metrics allow organizations to customize the score based on their unique environment.\\
The metrics produce a numerical score from 0 to 10, and the assessment is also represented as a vector string, which is a concise textual representation of the values used to calculate the score.~\cite{cvss-metrics}

% TODO: Add CVEs for year or some statistics in general

NVD is the \textit{National Vulnerability Database}, a U.S. government database that contains information about known vulnerabilities, including their CVE identifiers and CVSS scores. It is maintained by NIST and serves as a comprehensive resource for organizations to evaluate security risks.

NIST (\textit{National Institute of Standards and Technology}) is a U.S. federal agency that develops cybersecurity standards, guidelines and best practices. Even if it is an American agency, its standards are widely used around the world.

\subsection{Bug}

A bug is a flaw in a system that is not behaving as it is designed to do; a vulnerability is a way of abusing the system, in a security-related way, whether that's due to a design fault or an implementation fault, so a bug.

Many security issues are related to vulnerabilities due to bugs, but not all bugs are vulnerabilities. The exploitation is the activity composed by many steps in which an attacker uses a bug to gain its goal.

\section{IT and OT Security}

IT (\textit{Information Technology}) and OT (\textit{Operational Technology}) focus on protecting different types of systems and they have distinct priorities.

IT is primarily concerned with managing electronic data, supporting business operations and facilitating decision-making through the use of computers and software to securely gather, store, process and share information. In contrast, OT focuses on controlling physical processes and equipment, ensuring efficiency and safety in industrial operations such as manufacturing and energy. Unlike IT, OT directly interfaces with industrial machinery and processes, addressing the physical environment and operational requirements.~\cite{paloalto-it-ot-diff}

We can devise some comparison points between the two types:
\subsection{Primary priorities}

IT security prioritizes Confidentiality, Integrity and Availability (CIA), with the main focus set to protect data from breaches and authorized actions, while OT security prioritizes Availability, Integrity and Confidentiality (AIC), ensuring the continuous operation of critical systems with safety and reliability being more important than data confidentiality.

\subsection{Impact of Security Breaches}

IT breaches typically affect data confidentiality and may result in privacy violations, financial losses or business disruptions. OT breaches can have severe direct consequences, including physical damage and environmental harm.

\subsection{Security Measures}

IT security can use cybersecurity tools such as firewalls, antivirus, encryption and user authentication. Instead, OT security should require specialized tools like network segmentation, strict access control and real-time monitoring.

\subsection{Regulatory Requirements}

The former uses regulations like GDPR, the \textit{General Data Protection Regulation} to protect personal data for a European citizen and standards like ISO-27001, while the latter follows regulations and standards like the IEC 62443, which is a series of standards for industrial automation and control systems management and security. More on these regulations in the next chapters.

\subsection{Vulnerability management}

IT systems can usually be patched with a software update in a short time without severe impact on operations, while OT systems may require a longer time to be patched, as they are often critical to the operation of industrial processes or incompatible with the running legacy systems.

Despite their distinct differences, IT vs. OT cybersecurity share similarities and are increasingly overlapping and one approach has not to exclude the other.

According to the \textit{2023 State of Operational Technology and Cybersecurity Report} by Fortinet\footnote{\url{https://www.fortinet.com/}}, as shown in~\cref{fig:fortinet-intrusions-env-impacted}, nearly one-third of respondents indicated both IT and OT systems were impacted, up from the 21\% in 2022.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.8]{chapters/02/assets/fortinet-intrusions-env-impacted.png}
  \caption[Environments impacted]{Environments impacted}
  \label{fig:fortinet-intrusions-env-impacted}
\end{figure}


As a general rule, OT devices are traditionally kept separate from the public internet and often internal networks, which means they can only be accessed by authorized employees. However, it is increasingly possible for OT systems to be controlled and monitored by IT systems or remotely via the Internet.~\cite{it-vs-ot-cybersecurity}

\section{Industry 4.0}

Industry 4.0, synonymous of \textit{smart manufacturing}, is the realization of the digital transformation of the field, delivering real-time decision-making, enhanced productivity, flexibility and agility to revolutionize the way companies manufacture, improve and distribute their products.

This is the Fourth Industrial Revolution, characterized by increasing automation and the employment of smart machines and smart factories. By collecting more data from the factory floor and combining that with other enterprise operational data, a smart factory can achieve information transparency and better decisions.

Some specific technologies are pushing this revolution:~\cite{what-is-industry-4-0}
\begin{itemize}
  \item \textbf{Internet of Things (IoT)}: machines on the factory floor are equipped with boars that allow the machines to connect with remote services, making possible for large amounts of valuable data to be collected, analyzed and exchanged.
  \item \textbf{Cloud computing}: the typically large amount of data being stored and analyzed can be processed efficiently and cost-effectively with \textit{the cloud}. Cloud computing can also reduce startup costs for small- and medium-sized manufacturers who can right-size their needs and scale as their business grows.
  \item \textbf{AI and machine learning}: Artificial Intelligence (AI) and machine learning can create insights providing visibility, predictability and automation of operations and business processes. The data collected from these assets can help businesses perform predictive maintenance based on machine learning algorithms, resulting in more uptime and higher efficiency.
  \item \textbf{Edge computing}: to reduce latency and improve security, edge computing can be used to process the needed data closer to the source, rather than sending all the raw statistics to the cloud, wasting bandwidth and time.
  \item \textbf{Cybersecurity}: last but not least, when undergoing a digital transformation to Industry 4.0, it is essential to consider a cybersecurity approach that encompasses IT and OT equipment.
\end{itemize}

\section{Industrial devices: PLC and HMI}

Industrial devices are used in many different sectors like manufacturing, energy, transportation and so on. We can trace back these devices to a common tablet placed on a wall or on a production line, with the difference that these devices support a wide range of industrial protocols. These devices are usually not connected to the internet and they take input from a physical human interaction directly on the place.

This type of device is called HMI, which stands for \textit{Human Machine Interface}, and they are defined as a feature or component of a certain device or software application that enables humans to engage and interact with machines.\\
Traditionally, to integrate a manufacturing line with an HMI, the HMI had to be connected to a Programming Logic Controller (PLC) and the HMI displayed the data received from the PLC and gave the PLC input from users.~\cite{what-is-hmi}

In terms of the demands of Industry 4.0, industrial HMIs will also see further incorporation of new and emerging technologies that are impacting HMIs as a whole. First of all, there is the need to start integrating the Internet of Things (IoT) into industrial HMIs. This will allow for the collection of data from the factory floor and the ability to analyze that data in real time. This will also allow for the ability to control the factory floor from a remote location. Of course, making these devices connected to the internet makes the risks of cyberattacks higher.

\section{ICS and IACS}

ICS and IACS refer respectively to \textit{Industrial Control System} and \textit{Industrial Automation and Control Systems}. In our context, they can be used interchangeably to define the collection of hardware, software and policies that control and manage the industrial process. Basically, it means that anything interacting with the system influencing its safety, security and operations belongs to IACS.~\cite{ics-or-iacs}

We use this terminology in the next chapters, especially related to the security regulations of the systems.

\section{Security Scan}

A security scan is a process that looks for vulnerabilities in a system or network. It is a way to identify potential security risks and weaknesses that could be exploited by attackers. Security scans can be performed manually or automatically using specialized tools. The goal of a security scan is to identify vulnerabilities and provide recommendations for improving the security of the system.

There are different types of security scans, including network scans, vulnerability scans and penetration tests. Network scans are used to identify devices on a network and detect open ports and services. Vulnerability scans are used to identify security vulnerabilities in software and systems. Penetration tests are used to simulate cyberattacks and test the security of a system.

Security scans are an important part of a comprehensive security program. They can help organizations identify and address security vulnerabilities before they are exploited by attackers. By regularly performing security scans, organizations can improve their security attitude and reduce the risk of a security breach.

\subsection{Snyk}

Snyk\footnote{\url{https://snyk.io}} is a security company that provides a platform that enables developers to find and fix vulnerabilities in their code and dependencies. Snyk integrates with popular development tools and workflows, making it easy for developers to identify and address security issues early in the development process.

Snyk offers a range of security tools, including vulnerability scanning, dependency monitoring and container security. The platform provides real-time alerts and recommendations for fixing security issues, helping developers to secure their applications and prevent security breaches.

\section{JSON and YAML}

JSON, acronym of \textit{JavaScript Object Notation}, is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is based on a subset of the JavaScript programming language. JSON is often used to exchange data between a server and a web application. It is a text format that is language-independent and can be used with any programming language. JSON is commonly used for configuration files, API responses and data storage.

YAML, a recursive acronym of \textit{YAML Ain't Markup Language}, is a human-readable data serialization format that is usually used for configuration files. It is a superset of JSON and is designed to be more human-readable and easier to write than JSON. It is commonly used in the cloud computing industry for configuration files and infrastructure as code.

\begin{minipage}{\linewidth}
  \begin{lstlisting}[style=json, caption={JSON example}, label={lst:json-example}]
{
  "name": "John",
  "employed": true,
  "address": {
    "city": "Verona",
    "country": "Italy"
  },
  "hobby": ["reading", "running"]
}
  \end{lstlisting}
\end{minipage}

\begin{minipage}{\linewidth}
  \begin{lstlisting}[style=yaml, caption={YAML example}, label={lst:yaml-example}]
name: John
employed: true
address:
  city: Verona
  country: Italy
hobby:
  - reading
  - running
  \end{lstlisting}
\end{minipage}

The example in~\cref{lst:json-example} show the data represented in JSON, while~\cref{lst:yaml-example} shows the same data as YAML.

\section{REST API}

REST stands for \textit{REpresentational State Transfer}, and it is an architectural style for designing networked applications. It relies on a stateless client-server communication protocol, meaning that each request from a client to a server must contain all the information necessary to understand the request. REST is often used in web services development as a way to communicate between client and server.

A REST API is an application programming interface that uses HTTP requests to perform operations on a server. The endpoints can be called in a RESTful way, meaning that the user transfers a representation of the state of the resource to the requester or endpoint with appropriate HTTP methods to perform standard database functions like creating, reading, updating and deleting records (also known as \textit{CRUD}) within a resource.~\cite{rest-api}

\section{Deployment of the microservices}

This section explains how a software can be deployed in a cloud environment, with a focus on the microservices architecture.

\subsection{Microservice}

Microservice architecture is an architectural style that organizes an application into a set of services that can be deployed independently and are loosely coupled. This means each service is packaged as an executable unit ready for production and that the services do not have direct dependencies on each other, allowing for independent development, deployment and scaling.~\cite{microservices-what-are}

\subsection{Virtualization}

Virtualization is a technology that enables the creation of virtual versions of physical resources such as processors, storage devices, or network components. By simulating the functions of physical hardware, virtualization allows multiple operating systems to run on a single physical machine. This approach enhances IT agility, flexibility and scalability, while also providing significant cost savings by abstracting the underlying physical hardware.~\cite{virtualization-aws}

Each virtualized environment runs within its allocated resources. There are two main types of virtualization approaches: container-based virtualization and hypervisor-based virtualization, also known as virtual machines' virtualization.

\subsection{Container vs. Virtual Machine}

A modern and efficient way to execute microservices is to use \textit{containers}. \\
Containers are lightweight software packages that contain all the dependencies required to execute the contained software application in a virtualized environment. They share the same kernel of the host system, but they are usually isolated from the host and from other containers and they can run different operating systems. Another advantage of containers is that they are immutable, meaning that they could be altered after their creation, but the state is restored at each execution, without considering possible further different configurations.\\
Instead, compared to the containers, \textit{virtual machines}'s goal is to virtualize the entire system hardware, therefore they are slower and heavier - resources side and disk usage side - than containers.

The company uses \textit{Docker}\footnote{\url{https://www.docker.com}} to create, deploy and run containers. Docker is an application that allows developers to build, package and run applications as containers.

Docker itself enables the user to instantiate a single container at time from an image, that is a series of instructions that defines the environment of the container, starting from the base operating system up to all the steps needed to install the final software. Since we are talking of a microservices architecture, there is the need to run multiple containers at the same time, and to manage them, from the creation to the destruction up to the recovery in case of failure or the scaling in case of high load. This is where we need an orchestration tool.

\subsection{Orchestration}

The containers are managed by \textit{Kubernetes}\footnote{\url{https://kubernetes.io}}, also known as \textit{K8s}, an open-source container orchestration platform that provides a platform for automating the deployment, scaling and operations of application containers across clusters of hosts. It works with a range of container tools, including Docker. Kubernetes was originally developed by Google, and in 2014 it has been open-sourced.~\cite{kubernetes-overview}

Kubernetes is a powerful tool that allows the user to manage the containers in a declarative way, that is the user declares the desired state of the system, and Kubernetes takes care of the rest. The user can define the number of replicas of a container, the resources needed by the container, the network configuration, the storage configuration and much more. Kubernetes will take care of the deployment of the containers, the scaling of the containers, the recovery of the containers in case of failure and the load balancing of the containers.

An instance of Kubernetes is called a \textit{cluster} and it is composed of \textit{master nodes} and one or more \textit{worker nodes}. The master node is in charge of the orchestration of the containers, while the worker nodes are in charge of running the containers. The master node is composed of the API server, the scheduler, the controller manager and \textit{etcd}, that is a distributed key-value store used to store the state of the cluster. The worker nodes are composed of the \textit{kubelet}, which is the agent that runs on each node and is responsible for the communication between the master node and the worker node and the \textit{kube-proxy}, that is a network proxy that runs on each node and maintains network rules.

Every action towards the cluster is performed through the API calls to the API server, that is the entry point for the cluster. The user can interact with the cluster through the \textit{kubectl} command-line tool, that is the Kubernetes command-line tool, or through the many available third-party softwares that interact with the Kubernetes API.

\subsection{Helm}

\textit{Helm}\footnote{\url{https://helm.sh}} is a package manager for Kubernetes that allows the user to define, install and upgrade Kubernetes applications. Helm uses a packaging format called \textit{charts}, that are a collection of files that describe a set of Kubernetes resources. Helm charts can be used to define the resources needed by the application, the dependencies, the configuration and the hooks that should be executed during the installation.

Helm is used by the company to deploy the microservices on the Kubernetes cluster.

\subsection{CI/CD}

We said that to be instantiated a Docker container needs an image. The image is effectively the software to be executed, and the container is its running instance. The images must be built on the source code of the software and the building process can be automated by a \textit{Continuous Integration} (CI) tool. Later, the images must be pushed to a registry, that is a repository where the images are stored, and the deployment of the images can be automated by a \textit{Continuous Deployment} (CD) tool.

CI and CD are practices that allow the developers to automate the building, testing and deployment of the software. CI is the practice of integrating the code changes of the developers into a shared repository multiple times a day, and CD is the practice of automatically deploying the code changes to the production environment. CI/CD allows the developers to detect and fix the bugs early, to reduce the risk of integration issues and to deliver the software faster and more frequently.

The company uses a series of flows to automate the building, testing and deployment of the software.
Once the code is pushed to the main or to the developer branch of the repository, the CI tool, hosted on the Github Actions\footnote{\url{https://github.com/features/actions}}, automatically runs the tests and, if successful, builds the image for \texttt{amd64} and \texttt{arm64} and it pushes the image to a private container registry, hosted on Google Artifact Registry\footnote{\url{https://cloud.google.com/artifact-registry/docs}}.

% terraform?
